{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGhHAOaaaFLW"
      },
      "source": [
        "**13장 – 텐서플로에서 데이터 적재와 전처리하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFf9S26XaFLZ"
      },
      "source": [
        "_이 노트북에는 13장의 모든 샘플 코드와 연습 문제에 대한 솔루션이 포함되어 있습니다._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4FGZHK6aFLZ"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/rickiepark/handson-ml3/blob/main/13_loading_and_preprocessing_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXdtU6jXaFLa",
        "tags": []
      },
      "source": [
        "# 13.0 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9umfFk4haFLa"
      },
      "source": [
        "이 프로젝트에는 Python 3.7 이상이 필요합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou98Y8ZRaFLa"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Nj_p_MaFLb"
      },
      "source": [
        "또한 Scikit-Learn ≥ 1.0.1이 필요합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWJUlqglaFLb"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDWOPc10aFLb"
      },
      "source": [
        "그리고 TensorFlow ≥ 2.8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l754yJLdaFLc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7NS9wsIaFLc"
      },
      "source": [
        "# 13.1 tf.data API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "didi0oqfaFLc",
        "outputId": "b8aef0c5-92a1-47e2-d409-53dc0c71130d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "X = tf.range(10)  # 데이터 텐서\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFYXeC4zaFLd",
        "outputId": "3d855c14-1b87-4a27-f8af-a79c7bb17d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNFzdsXQaFLd",
        "outputId": "d0eeb4eb-1cc8-4632-d68b-39508fe4b421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n",
            "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n",
            "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n"
          ]
        }
      ],
      "source": [
        "X_nested = {\"a\": ([1, 2, 3], [4, 5, 6]), \"b\": [7, 8, 9]}\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mECmZC1RaFLd"
      },
      "source": [
        "## 13.1.1 연쇄 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL47762maFLd",
        "outputId": "975125be-165b-4314-ab11-2477a8e25a24",
        "tags": [
          "raises-exception"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
        "dataset = dataset.repeat(3).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6CLQRJvaFLe",
        "outputId": "62901aa8-4d36-491a-a321-062eb8e9e2bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
            "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.map(lambda x: x * 2)  # x는 배치\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIAoP4uZaFLe",
        "outputId": "b79e36b1-3c98-4104-c39b-edaabbd4ac44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ6eYR53aFLe",
        "outputId": "001b3c23-71ca-415d-b1ce-13fa33e1d03a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for item in dataset.take(2):\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0avrLKyvaFLe"
      },
      "source": [
        "## 13.1.2 데이터 셔플링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgIdlBIoaFLe",
        "outputId": "3674c70d-267b-43d7-be50-670b9d3ddd7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)\n",
            "tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "dataset = tf.data.Dataset.range(10).repeat(2)\n",
        "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIrz0WqvaFLf"
      },
      "source": [
        "## 13.1.3 여러 파일에서 한 줄씩 번갈아 읽기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1BqYTOCaFLf"
      },
      "source": [
        "캘리포니아 주택 데이터셋을 로드하고 준비해 보겠습니다. 먼저 데이터셋을 로드한 다음 훈련 세트, 검증 세트, 테스트 세트로 분할합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWjtg6vtaFLf"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 캘리포니아 주택 데이터셋을 가져오고, 분할하고, 정규화합니다.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ahgXYx9aFLf"
      },
      "source": [
        "메모리에 맞지 않는 매우 큰 데이터셋의 경우, 일반적으로 먼저 여러 파일로 분할한 다음 텐서플로가 이 파일들을 병렬로 읽도록 합니다. 이를 시연하기 위해 주택 데이터셋을 분할하여 20개의 CSV 파일로 저장하겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C8lnoLZaFLf"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 데이터셋을 20개 파일로 분할하여 CSV 파일로 저장합니다.\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def save_to_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "    housing_dir = Path() / \"datasets\" / \"housing\"\n",
        "    housing_dir.mkdir(parents=True, exist_ok=True)\n",
        "    filename_format = \"my_{}_{:02d}.csv\"\n",
        "\n",
        "    filepaths = []\n",
        "    m = len(data)\n",
        "    chunks = np.array_split(np.arange(m), n_parts)\n",
        "    for file_idx, row_indices in enumerate(chunks):\n",
        "        part_csv = housing_dir / filename_format.format(name_prefix, file_idx)\n",
        "        filepaths.append(str(part_csv))\n",
        "        with open(part_csv, \"w\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header)\n",
        "                f.write(\"\\n\")\n",
        "            for row_idx in row_indices:\n",
        "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
        "                f.write(\"\\n\")\n",
        "    return filepaths\n",
        "\n",
        "train_data = np.c_[X_train, y_train]\n",
        "valid_data = np.c_[X_valid, y_valid]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = \",\".join(header_cols)\n",
        "\n",
        "train_filepaths = save_to_csv_files(train_data, \"train\", header, n_parts=20)\n",
        "valid_filepaths = save_to_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
        "test_filepaths = save_to_csv_files(test_data, \"test\", header, n_parts=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkMSu9PbaFLf"
      },
      "source": [
        "이제 이 CSV 파일 중 하나의 처음 몇 줄을 살펴 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0oK8PDeaFLg",
        "outputId": "6ed8f7c3-97ce-4d42-ea71-6a8b6e4d2a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
            "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
            "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
            "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\".join(open(train_filepaths[0]).readlines()[:4]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTwAr87raFLg",
        "outputId": "eea48beb-09f0-4bdd-eb40-681aaeb2424a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datasets/housing/my_train_00.csv',\n",
              " 'datasets/housing/my_train_01.csv',\n",
              " 'datasets/housing/my_train_02.csv',\n",
              " 'datasets/housing/my_train_03.csv',\n",
              " 'datasets/housing/my_train_04.csv',\n",
              " 'datasets/housing/my_train_05.csv',\n",
              " 'datasets/housing/my_train_06.csv',\n",
              " 'datasets/housing/my_train_07.csv',\n",
              " 'datasets/housing/my_train_08.csv',\n",
              " 'datasets/housing/my_train_09.csv',\n",
              " 'datasets/housing/my_train_10.csv',\n",
              " 'datasets/housing/my_train_11.csv',\n",
              " 'datasets/housing/my_train_12.csv',\n",
              " 'datasets/housing/my_train_13.csv',\n",
              " 'datasets/housing/my_train_14.csv',\n",
              " 'datasets/housing/my_train_15.csv',\n",
              " 'datasets/housing/my_train_16.csv',\n",
              " 'datasets/housing/my_train_17.csv',\n",
              " 'datasets/housing/my_train_18.csv',\n",
              " 'datasets/housing/my_train_19.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "train_filepaths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfYB-9HWaFLg"
      },
      "source": [
        "**입력 파이프라인 구축하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG7LhaDWaFLg"
      },
      "outputs": [],
      "source": [
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGyOUGytaFLg",
        "outputId": "88fd9ce3-60db-4469-f2e0-e1f3b698d07a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# 추가 코드 - 파일 경로가 섞여 있음을 보여줍니다.\n",
        "for filepath in filepath_dataset:\n",
        "    print(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oaF45toaFLg"
      },
      "outputs": [],
      "source": [
        "n_readers = 5\n",
        "dataset = filepath_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length=n_readers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1ELUJQzaFLg",
        "outputId": "ed9ca856-1904-4de5-b628-f51d19491405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'4.5909,16.0,5.475877192982456,1.0964912280701755,1357.0,2.9758771929824563,33.63,-117.71,2.418', shape=(), dtype=string)\n",
            "tf.Tensor(b'2.4792,24.0,3.4547038327526134,1.1341463414634145,2251.0,3.921602787456446,34.18,-118.38,2.0', shape=(), dtype=string)\n",
            "tf.Tensor(b'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67', shape=(), dtype=string)\n",
            "tf.Tensor(b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205', shape=(), dtype=string)\n",
            "tf.Tensor(b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for line in dataset.take(5):\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SftQ44b5aFLh"
      },
      "source": [
        "## 13.1.4 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "o9US4r4NaFLh",
        "outputId": "0585984b-6c0d-41e6-9b25-5cf71e3e63c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# 추가 코드 - 각 특성의 평균 및 표준 편차 계산\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TYEAH5oaFLh"
      },
      "outputs": [],
      "source": [
        "X_mean, X_std = scaler.mean_, scaler.scale_  # 추가 코드\n",
        "n_inputs = 8\n",
        "\n",
        "def parse_csv_line(line):\n",
        "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
        "\n",
        "def preprocess(line):\n",
        "    x, y = parse_csv_line(line)\n",
        "    return (x - X_mean) / X_std, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK2lTZO-aFLh",
        "outputId": "5a1c6280-6864-4c80-8878-7b58f02b5d3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
              " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
              "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPmr1fvBaFLh"
      },
      "source": [
        "## 13.1.5 데이터 적재와 전처리를 합치기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8Ha8wpWaFLh"
      },
      "outputs": [],
      "source": [
        "def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None,\n",
        "                       n_parse_threads=5, shuffle_buffer_size=10_000, seed=42,\n",
        "                       batch_size=32):\n",
        "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
        "    return dataset.batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4eU9VOgaFLh",
        "outputId": "0f2dbe4c-f2df-4ceb-f857-533e00923b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X = tf.Tensor(\n",
            "[[-1.3957452  -0.04940685 -0.22830808  0.22648273  2.2593622   0.35200632\n",
            "   0.9667386  -1.4121602 ]\n",
            " [ 2.7112627  -1.0778131   0.69413143 -0.14870553  0.51810503  0.3507294\n",
            "  -0.82285154  0.80680597]\n",
            " [-0.13484643 -1.868895    0.01032507 -0.13787179 -0.12893449  0.03143518\n",
            "   0.2687057   0.13212144]], shape=(3, 8), dtype=float32)\n",
            "y = tf.Tensor(\n",
            "[[1.819]\n",
            " [3.674]\n",
            " [0.954]], shape=(3, 1), dtype=float32)\n",
            "\n",
            "X = tf.Tensor(\n",
            "[[ 0.09031774  0.9789995   0.1327582  -0.13753782 -0.23388447  0.10211545\n",
            "   0.97610843 -1.4121602 ]\n",
            " [ 0.05218809 -2.0271113   0.2940109  -0.02403445  0.16218767 -0.02844518\n",
            "   1.4117942  -0.93737936]\n",
            " [-0.672276    0.02970133 -0.76922584 -0.15086786  0.4962024  -0.02741998\n",
            "  -0.7853724   0.77182245]], shape=(3, 8), dtype=float32)\n",
            "y = tf.Tensor(\n",
            "[[2.725]\n",
            " [1.205]\n",
            " [1.625]], shape=(3, 1), dtype=float32)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 추가 코드 - 데이터셋에서 생성된 처음 몇 개의 배치를 출력합니다.\n",
        "\n",
        "example_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
        "for X_batch, y_batch in example_set.take(2):\n",
        "    print(\"X =\", X_batch)\n",
        "    print(\"y =\", y_batch)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQkb9HYEaFLh"
      },
      "source": [
        "다음은 `Dataset` 클래스의 각 메서드에 대한 간단한 설명입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "219yjuicaFLi",
        "outputId": "08b67f52-3e6c-4c44-e46f-dd0d35c249a8",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "● apply()              Applies a transformation function to this dataset.\n",
            "● as_numpy_iterator()  Returns an iterator which converts all elements of the dataset to numpy.\n",
            "● batch()              Combines consecutive elements of this dataset into batches.\n",
            "● bucket_by_sequence_length()A transformation that buckets elements in a `Dataset` by length.\n",
            "● cache()              Caches the elements in this dataset.\n",
            "● cardinality()        Returns the cardinality of the dataset, if known.\n",
            "● choose_from_datasets()Creates a dataset that deterministically chooses elements from `datasets`.\n",
            "● concatenate()        Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
            "● counter()            Creates a `Dataset` that counts from `start` in steps of size `step`.\n",
            "● element_spec()       The type specification of an element of this dataset.\n",
            "● enumerate()          Enumerates the elements of this dataset.\n",
            "● filter()             Filters this dataset according to `predicate`.\n",
            "● fingerprint()        Computes the fingerprint of this `Dataset`.\n",
            "● flat_map()           Maps `map_func` across this dataset and flattens the result.\n",
            "● from_generator()     Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
            "● from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors.\n",
            "● from_tensors()       Creates a `Dataset` with a single element, comprising the given tensors.\n",
            "● get_single_element() Returns the single element of the `dataset`.\n",
            "● group_by_window()    Groups windows of elements by key and reduces them.\n",
            "● ignore_errors()      Drops elements that cause errors.\n",
            "● interleave()         Maps `map_func` across this dataset, and interleaves the results.\n",
            "● list_files()         A dataset of all files matching one or more glob patterns.\n",
            "● load()               Loads a previously saved dataset.\n",
            "● map()                Maps `map_func` across the elements of this dataset.\n",
            "● options()            Returns the options for this dataset and its inputs.\n",
            "● padded_batch()       Combines consecutive elements of this dataset into padded batches.\n",
            "● prefetch()           Creates a `Dataset` that prefetches elements from this dataset.\n",
            "● ragged_batch()       Combines consecutive elements of this dataset into `tf.RaggedTensor`s.\n",
            "● random()             Creates a `Dataset` of pseudorandom values.\n",
            "● range()              Creates a `Dataset` of a step-separated range of values.\n",
            "● rebatch()            Creates a `Dataset` that rebatches the elements from this dataset.\n",
            "● reduce()             Reduces the input dataset to a single element.\n",
            "● rejection_resample() Resamples elements to reach a target distribution.\n",
            "● repeat()             Repeats this dataset so each original value is seen `count` times.\n",
            "● sample_from_datasets()Samples elements at random from the datasets in `datasets`.\n",
            "● save()               Saves the content of the given dataset.\n",
            "● scan()               A transformation that scans a function across an input dataset.\n",
            "● shard()              Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
            "● shuffle()            Randomly shuffles the elements of this dataset.\n",
            "● skip()               Creates a `Dataset` that skips `count` elements from this dataset.\n",
            "● snapshot()           API to persist the output of the input dataset.\n",
            "● sparse_batch()       Combines consecutive elements into `tf.sparse.SparseTensor`s.\n",
            "● take()               Creates a `Dataset` with at most `count` elements from this dataset.\n",
            "● take_while()         A transformation that stops dataset iteration based on a `predicate`.\n",
            "● unbatch()            Splits elements of a dataset into multiple elements.\n",
            "● unique()             A transformation that discards duplicate elements of a `Dataset`.\n",
            "● window()             Returns a dataset of \"windows\".\n",
            "● with_options()       Returns a new `tf.data.Dataset` with the given options set.\n",
            "● zip()                Creates a `Dataset` by zipping together the given datasets.\n"
          ]
        }
      ],
      "source": [
        "# extra code – list all methods of the tf.data.Dataset class\n",
        "for m in dir(tf.data.Dataset):\n",
        "    if not (m.startswith(\"_\") or m.endswith(\"_\")):\n",
        "        func = getattr(tf.data.Dataset, m)\n",
        "        if hasattr(func, \"__doc__\"):\n",
        "            print(\"● {:21s}{}\".format(m + \"()\", func.__doc__.split(\"\\n\")[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qozq_WGwaFLi"
      },
      "source": [
        "## 13.1.7 케라스와 데이터셋 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvxZ12U_aFLi"
      },
      "outputs": [],
      "source": [
        "train_set = csv_reader_dataset(train_filepaths)\n",
        "valid_set = csv_reader_dataset(valid_filepaths)\n",
        "test_set = csv_reader_dataset(test_filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QevYvK7paFLi"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 재현성을 위한\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXjoP1YIaFLi",
        "outputId": "28d80e57-c1bd-4b20-85fa-ca0fab3517eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "    363/Unknown \u001b[1m2s\u001b[0m 2ms/step - loss: 1.5630"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 1.5619 - val_loss: 8.2494\n",
            "Epoch 2/5\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.5572 - val_loss: 0.5753\n",
            "Epoch 3/5\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4890 - val_loss: 2.2969\n",
            "Epoch 4/5\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4629 - val_loss: 0.8388\n",
            "Epoch 5/5\n",
            "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.4903 - val_loss: 0.6413\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c17d06ffd30>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
        "                          input_shape=X_train.shape[1:]),\n",
        "    tf.keras.layers.Dense(1),\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
        "model.fit(train_set, validation_data=valid_set, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clRmPdmzaFLi",
        "outputId": "160a9b91-cca1-420d-f125-cddb0fd11612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4089\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        }
      ],
      "source": [
        "test_mse = model.evaluate(test_set)\n",
        "new_set = test_set.take(3)  # 3개의 새 샘플이 있다고 가정합니다.\n",
        "y_pred = model.predict(new_set)  # 그냥 NumPy 배열을 전달할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQw5XVWfaFLi",
        "outputId": "50f39a52-8ab8-4755-aa11-630090603a50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5"
          ]
        }
      ],
      "source": [
        "# 추가 코드 - 훈련을 위한 옵티마이저 및 손실 함수를 정의합니다.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "n_epochs = 5\n",
        "for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in train_set:\n",
        "        # 추가 코드 - 12장에 설명된 대로 경사 하강법 스텝 하나를 수행합니다.\n",
        "        print(\"\\rEpoch {}/{}\".format(epoch + 1, n_epochs), end=\"\")\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch)\n",
        "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "            loss = tf.add_n([main_loss] + model.losses)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTgD2O7uaFLi",
        "outputId": "bbe9e924-b2d0-4ed4-a07d-7df12201f8af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5"
          ]
        }
      ],
      "source": [
        "@tf.function\n",
        "def train_one_epoch(model, optimizer, loss_fn, train_set):\n",
        "    for X_batch, y_batch in train_set:\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch)\n",
        "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "            loss = tf.add_n([main_loss] + model.losses)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    print(\"\\rEpoch {}/{}\".format(epoch + 1, n_epochs), end=\"\")\n",
        "    train_one_epoch(model, optimizer, loss_fn, train_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijiubaOzaFLj"
      },
      "source": [
        "# 13.2 TFRecord 포맷"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udeK-m5JaFLj"
      },
      "source": [
        "TFRecord 파일은 이진 레코드의 목록입니다. `tf.io.TFRecordWriter`를 사용하여 생성할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwXRItn5aFLj"
      },
      "outputs": [],
      "source": [
        "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
        "    f.write(b\"This is the first record\")\n",
        "    f.write(b\"And this is the second record\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S-jBIFVaFLj"
      },
      "source": [
        "그리고 `tf.data.TFRecordDataset`을 사용하여 읽을 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLvnUbBAaFLj",
        "outputId": "057d19eb-ad36-47f3-98b8-ecce0b17d6d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
            "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "filepaths = [\"my_data.tfrecord\"]\n",
        "dataset = tf.data.TFRecordDataset(filepaths)\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf18Ir6IaFLj"
      },
      "source": [
        "하나의 `TFRecordDataset`으로 여러 개의 TFRecord 파일을 읽을 수 있습니다. 기본적으로 한 번에 하나씩 읽지만 `num_parallel_reads=3`을 설정하면 한 번에 3개씩 병렬로 읽고 레코드를 인터리빙합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExXpC4nTaFLj",
        "outputId": "1715ef1e-7117-4584-e334-f39682de80c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'File 0 record 0', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 1 record 0', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 2 record 0', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 0 record 1', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 1 record 1', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 2 record 1', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 0 record 2', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 1 record 2', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 2 record 2', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 3 record 0', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 4 record 0', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 3 record 1', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 4 record 1', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 3 record 2', shape=(), dtype=string)\n",
            "tf.Tensor(b'File 4 record 2', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# 추가 코드 - 여러 파일을 병렬로 읽고 인터리브하는 방법을 보여줍니다.\n",
        "\n",
        "filepaths = [\"my_test_{}.tfrecord\".format(i) for i in range(5)]\n",
        "for i, filepath in enumerate(filepaths):\n",
        "    with tf.io.TFRecordWriter(filepath) as f:\n",
        "        for j in range(3):\n",
        "            f.write(\"File {} record {}\".format(i, j).encode(\"utf-8\"))\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=3)\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yI9qE07aFLj"
      },
      "source": [
        "## 13.2.1 압축된 TFRecord 파일"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZPq7aVIaFLk"
      },
      "outputs": [],
      "source": [
        "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
        "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
        "    f.write(b\"Compress, compress, compress!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyaz16b_aFLk"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
        "                                  compression_type=\"GZIP\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePaZy4vaaFLk",
        "outputId": "aa06a00c-2a61-405f-b2e2-2e88436ffa32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'Compress, compress, compress!', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# extra code – shows that the data is decompressed correctly\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O46_kmZqaFLk"
      },
      "source": [
        "## 13.2.2 프로토콜 버퍼 개요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdqX3b7TaFLk"
      },
      "source": [
        "이 섹션에서는 [프로토콜 버퍼 설치](https://developers.google.com/protocol-buffers/docs/downloads)가 필요합니다. 일반적으로 텐서플로에는 `tf.train.Example` 유형의 프로토콜 버퍼를 생성하고 구문 분석하는 함수가 함께 제공되므로 일반적으로는 이렇게 할 필요가 없습니다. 하지만 이 섹션에서는 간단한 프로토버프 정의를 직접 만들어 프로토콜 버퍼에 대해 배우기 때문에 프로토버프 컴파일러(`protoc`)가 필요합니다. 이 컴파일러를 사용해 프로토버프 정의를 파이썬 모듈로 컴파일한 다음 코드에서 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdDcbKm9aFLk"
      },
      "source": [
        "먼저 간단한 프로토버프 정의를 작성해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRbM0IURaFLk",
        "outputId": "fcdfbad0-7c2b-4fc4-9d9e-667ef63638e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing person.proto\n"
          ]
        }
      ],
      "source": [
        "%%writefile person.proto\n",
        "syntax = \"proto3\";\n",
        "message Person {\n",
        "    string name = 1;\n",
        "    int32 id = 2;\n",
        "    repeated string email = 3;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41DK2BjMaFLk"
      },
      "source": [
        "그리고 컴파일해 보겠습니다(`--descriptor_set_out` 및 `--include_imports` 옵션은 아래 `tf.io.decode_proto()` 예시에만 필요합니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k86PBKVaFLk"
      },
      "outputs": [],
      "source": [
        "!protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTfZEYyJaFLk",
        "outputId": "8925b7fd-82a1-4143-bf73-af91f9de3646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "person.desc  person_pb2.py  person.proto\n"
          ]
        }
      ],
      "source": [
        "%ls person*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade protobuf\n",
        "!protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSHYxu7KCBIB",
        "outputId": "a3e7168e-e92d-4752-aa83-8c69523fef7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (4.25.5)\n",
            "Collecting protobuf\n",
            "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-5.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y protobuf-compiler\n",
        "!protoc --version # Check the installed version, it should be >= 3.19.0\n",
        "\n",
        "# Regenerate person_pb2.py\n",
        "!protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd-AfubCCrg_",
        "outputId": "5942a905-c3b3-44d4-e71f-3175291d6980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,190 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,523 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,514 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [35.0 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,225 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,446 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,738 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,454 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [53.3 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,625 kB]\n",
            "Fetched 24.2 MB in 6s (4,392 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "protobuf-compiler is already the newest version (3.12.4-1ubuntu7.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "libprotoc 3.12.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "5HjgLc4laFLl",
        "outputId": "e4590231-6f64-412c-84de-a10dbf22c9ca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-a242d017761a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mperson_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPerson\u001b[0m  \u001b[0;31m# 생성된 액세스 클래스 가져오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mperson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Al\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"a@b.com\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Person 만들기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Person 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/person_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mcreate_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_create_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   fields=[\n\u001b[0;32m---> 36\u001b[0;31m     _descriptor.FieldDescriptor(\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Person.name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpp_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/protobuf/descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    551\u001b[0m   \u001b[0mTYPE_BYTES\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m   \u001b[0mTYPE_UINT32\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m   \u001b[0mTYPE_ENUM\u001b[0m           \u001b[0;34m=\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m   \u001b[0mTYPE_SFIXED32\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0mTYPE_SFIXED64\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
          ]
        }
      ],
      "source": [
        "from person_pb2 import Person  # 생성된 액세스 클래스 가져오기\n",
        "\n",
        "person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # Person 만들기\n",
        "print(person)  # Person 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "JxK0rm94aFLl",
        "outputId": "9d5e107e-cbfb-42f3-8d30-82a4927deb88"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'person' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-3980f2ee9963>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m  \u001b[0;31m# 필드 읽기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'person' is not defined"
          ]
        }
      ],
      "source": [
        "person.name  # 필드 읽기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qYDccIgaFLl"
      },
      "outputs": [],
      "source": [
        "person.name = \"Alice\"  # 필드 수정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXNFw1lKaFLl"
      },
      "outputs": [],
      "source": [
        "person.email[0]  # 반복되는 필드는 배열처럼 액세스 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rHjXbJHaFLl"
      },
      "outputs": [],
      "source": [
        "person.email.append(\"c@d.com\")  # 이메일 주소 추가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1thXwLKeaFLl"
      },
      "outputs": [],
      "source": [
        "serialized = person.SerializeToString()  # person을 바이트 문자열로 직렬화\n",
        "serialized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-cY-oJlaFLl"
      },
      "outputs": [],
      "source": [
        "person2 = Person()  # 새로운 Person 만들기\n",
        "person2.ParseFromString(serialized)  # 바이트 문자열 파싱하기(27바이트 길이)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2oF3IInaFLl"
      },
      "outputs": [],
      "source": [
        "person == person2  # 두 값은 같습니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E3PgkQMaFLl"
      },
      "source": [
        "### 사용자 정의 프로토콜 버퍼"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EESYmfQaFLm"
      },
      "source": [
        "드물지만, 방금 만든 것과 같은 사용자 정의 프로토콜 버퍼를 텐서플로에서 파싱하고 싶을 수도 있습니다. 이를 위해 `tf.io.decode_proto()` 함수를 사용할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN4oI95NaFLm"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - tf.io.decode_proto() 함수를 사용하는 방법을 보여줍니다.\n",
        "\n",
        "person_tf = tf.io.decode_proto(\n",
        "    bytes=serialized,\n",
        "    message_type=\"Person\",\n",
        "    field_names=[\"name\", \"id\", \"email\"],\n",
        "    output_types=[tf.string, tf.int32, tf.string],\n",
        "    descriptor_source=\"person.desc\")\n",
        "\n",
        "person_tf.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82N5PK_GaFLm"
      },
      "source": [
        "자세한 내용은 [`tf.io.decode_proto()`](https://www.tensorflow.org/api_docs/python/tf/io/decode_proto) 설명서를 참조하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxk1x-YaaFLm"
      },
      "source": [
        "## 13.2.3 텐서플로 프로토콜 버퍼"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C783enPyaFLm"
      },
      "source": [
        "다음은 tf.train.Example 프로토콜 버퍼의 정의입니다:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be2fpO8DaFLm"
      },
      "source": [
        "```proto\n",
        "syntax = \"proto3\";\n",
        "\n",
        "message BytesList { repeated bytes value = 1; }\n",
        "message FloatList { repeated float value = 1 [packed = true]; }\n",
        "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
        "message Feature {\n",
        "    oneof kind {\n",
        "        BytesList bytes_list = 1;\n",
        "        FloatList float_list = 2;\n",
        "        Int64List int64_list = 3;\n",
        "    }\n",
        "};\n",
        "message Features { map<string, Feature> feature = 1; };\n",
        "message Example { Features features = 1; };\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_tsDlhRaFLm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Feature, Features, Example\n",
        "\n",
        "person_example = Example(\n",
        "    features=Features(\n",
        "        feature={\n",
        "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
        "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
        "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\n",
        "                                                          b\"c@d.com\"]))\n",
        "        }))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLmZxzQ9aFLm"
      },
      "outputs": [],
      "source": [
        "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
        "    for _ in range(5):\n",
        "        f.write(person_example.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xxPMnBFaFLm"
      },
      "source": [
        "## 13.2.4 Example 프로토콜 버퍼 읽고 파싱하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTtztPkSaFLn"
      },
      "outputs": [],
      "source": [
        "feature_description = {\n",
        "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
        "}\n",
        "\n",
        "def parse(serialized_example):\n",
        "    return tf.io.parse_single_example(serialized_example, feature_description)\n",
        "\n",
        "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).map(parse)\n",
        "for parsed_example in dataset:\n",
        "    print(parsed_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHP_ErzLaFLn"
      },
      "outputs": [],
      "source": [
        "tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkqHJICAaFLn"
      },
      "outputs": [],
      "source": [
        "parsed_example[\"emails\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQi1a_mBaFLn"
      },
      "outputs": [],
      "source": [
        "def parse(serialized_examples):\n",
        "    return tf.io.parse_example(serialized_examples, feature_description)\n",
        "\n",
        "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(2).map(parse)\n",
        "for parsed_examples in dataset:\n",
        "    print(parsed_examples)  # two examples at a time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP4zHklUaFLn"
      },
      "outputs": [],
      "source": [
        "parsed_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbQx2KXqaFLn"
      },
      "source": [
        "## 추가 자료 - 이미지와 텐서를 TFRecord에 저장하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvjLBSecaFLn"
      },
      "source": [
        "예제 이미지를 로드하고 표시해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEKSiCJdaFLn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_sample_images\n",
        "\n",
        "img = load_sample_images()[\"images\"][0]\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original Image\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FADh_AVDaFLn"
      },
      "source": [
        "이제 JPEG로 인코딩된 이미지가 포함된 `Example` 프로토버프를 만들어 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9y2oxfoaFLo"
      },
      "outputs": [],
      "source": [
        "data = tf.io.encode_jpeg(img)\n",
        "example_with_image = Example(features=Features(feature={\n",
        "    \"image\": Feature(bytes_list=BytesList(value=[data.numpy()]))}))\n",
        "serialized_example = example_with_image.SerializeToString()\n",
        "with tf.io.TFRecordWriter(\"my_image.tfrecord\") as f:\n",
        "    f.write(serialized_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOnXrKDFaFLo"
      },
      "source": [
        "마지막으로, 이 TFRecord 파일을 읽고, 각 `Example` 프로토콜 버퍼(이 경우 하나만)를 파싱하고, 예제에 포함된 이미지를 파싱하여 표시하는 tf.data 파이프라인을 만들어 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTdFnft3aFLo"
      },
      "outputs": [],
      "source": [
        "feature_description = { \"image\": tf.io.VarLenFeature(tf.string) }\n",
        "\n",
        "def parse(serialized_example):\n",
        "    example_with_image = tf.io.parse_single_example(serialized_example,\n",
        "                                                    feature_description)\n",
        "    return tf.io.decode_jpeg(example_with_image[\"image\"].values[0])\n",
        "    # 또는 tf.io.decode_image()를 대신 사용할 수 있습니다.\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(\"my_image.tfrecord\").map(parse)\n",
        "for image in dataset:\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLunOOZpaFLo"
      },
      "source": [
        "또는 BMP, GIF, JPEG, PNG 형식을 지원하는 `decode_image()`를 사용하세요:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKFpmMbpaFLo"
      },
      "source": [
        "텐서는 `tf.io.serialize_tensor()` 및 `tf.io.parse_tensor()`를 사용하여 쉽게 직렬화 및 파싱할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt0PtzppaFLo"
      },
      "outputs": [],
      "source": [
        "tensor = tf.constant([[0., 1.], [2., 3.], [4., 5.]])\n",
        "serialized = tf.io.serialize_tensor(tensor)\n",
        "serialized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwtApHzAaFLo"
      },
      "outputs": [],
      "source": [
        "tf.io.parse_tensor(serialized, out_type=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vVLQ56WaFLo"
      },
      "outputs": [],
      "source": [
        "sparse_tensor = parsed_example[\"emails\"]\n",
        "serialized_sparse = tf.io.serialize_sparse(sparse_tensor)\n",
        "serialized_sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njB_KgdhaFLo"
      },
      "outputs": [],
      "source": [
        "BytesList(value=serialized_sparse.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdTI3LgDaFLp"
      },
      "source": [
        "## 13.2.5 `SequenceExample` 프로토콜 버퍼를 사용해 리스트의 리스트 다루기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV5VTPCraFLp"
      },
      "source": [
        "```proto\n",
        "syntax = \"proto3\";\n",
        "\n",
        "message FeatureList { repeated Feature feature = 1; };\n",
        "message FeatureLists { map<string, FeatureList> feature_list = 1; };\n",
        "message SequenceExample {\n",
        "    Features context = 1;\n",
        "    FeatureLists feature_lists = 2;\n",
        "};\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-wgogjNaFLp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.train import FeatureList, FeatureLists, SequenceExample\n",
        "\n",
        "context = Features(feature={\n",
        "    \"author_id\": Feature(int64_list=Int64List(value=[123])),\n",
        "    \"title\": Feature(bytes_list=BytesList(value=[b\"A\", b\"desert\", b\"place\", b\".\"])),\n",
        "    \"pub_date\": Feature(int64_list=Int64List(value=[1623, 12, 25]))\n",
        "})\n",
        "\n",
        "content = [[\"When\", \"shall\", \"we\", \"three\", \"meet\", \"again\", \"?\"],\n",
        "           [\"In\", \"thunder\", \",\", \"lightning\", \",\", \"or\", \"in\", \"rain\", \"?\"]]\n",
        "comments = [[\"When\", \"the\", \"hurlyburly\", \"'s\", \"done\", \".\"],\n",
        "            [\"When\", \"the\", \"battle\", \"'s\", \"lost\", \"and\", \"won\", \".\"]]\n",
        "\n",
        "def words_to_feature(words):\n",
        "    return Feature(bytes_list=BytesList(value=[word.encode(\"utf-8\")\n",
        "                                               for word in words]))\n",
        "\n",
        "content_features = [words_to_feature(sentence) for sentence in content]\n",
        "comments_features = [words_to_feature(comment) for comment in comments]\n",
        "\n",
        "sequence_example = SequenceExample(\n",
        "    context=context,\n",
        "    feature_lists=FeatureLists(feature_list={\n",
        "        \"content\": FeatureList(feature=content_features),\n",
        "        \"comments\": FeatureList(feature=comments_features)\n",
        "    }))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM85hd9WaFLp"
      },
      "outputs": [],
      "source": [
        "sequence_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5tf-0C_aFLp"
      },
      "outputs": [],
      "source": [
        "serialized_sequence_example = sequence_example.SerializeToString()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG4xkg5ZaFLp"
      },
      "outputs": [],
      "source": [
        "context_feature_descriptions = {\n",
        "    \"author_id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    \"title\": tf.io.VarLenFeature(tf.string),\n",
        "    \"pub_date\": tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0]),\n",
        "}\n",
        "sequence_feature_descriptions = {\n",
        "    \"content\": tf.io.VarLenFeature(tf.string),\n",
        "    \"comments\": tf.io.VarLenFeature(tf.string),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUzLjTwFaFLp"
      },
      "outputs": [],
      "source": [
        "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
        "    serialized_sequence_example, context_feature_descriptions,\n",
        "    sequence_feature_descriptions)\n",
        "parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSrKhnvLaFLp"
      },
      "outputs": [],
      "source": [
        "parsed_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7xApOb1aFLp"
      },
      "outputs": [],
      "source": [
        "parsed_context[\"title\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_ZgIZy8aFLp"
      },
      "outputs": [],
      "source": [
        "parsed_feature_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8lRmFXvaFLq"
      },
      "outputs": [],
      "source": [
        "print(tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwg1V6oWaFLq"
      },
      "source": [
        "# 13.3 케라스 전처리 층"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKzc-e2waFLq"
      },
      "source": [
        "## 13.3.1 `Normalization` 층"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgXEGtpHaFLq"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - 재현성 보장\n",
        "norm_layer = tf.keras.layers.Normalization()\n",
        "model = tf.keras.models.Sequential([\n",
        "    norm_layer,\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
        "norm_layer.adapt(X_train)  # 모든 특성의 평균과 분산을 계산합니다.\n",
        "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JRCZjWLaFLq"
      },
      "outputs": [],
      "source": [
        "norm_layer = tf.keras.layers.Normalization()\n",
        "norm_layer.adapt(X_train)\n",
        "X_train_scaled = norm_layer(X_train)\n",
        "X_valid_scaled = norm_layer(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPBCitgTaFLq"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - 재현성 보장\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
        "model.fit(X_train_scaled, y_train, epochs=5,\n",
        "          validation_data=(X_valid_scaled, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4Vts0aVaFLq"
      },
      "outputs": [],
      "source": [
        "final_model = tf.keras.Sequential([norm_layer, model])\n",
        "X_new = X_test[:3]  # (스케일링되지 않은) 새 인스턴스가 몇 개 있다고 가정합니다.\n",
        "y_pred = final_model(X_new)  # 데이터를 전처리하고 예측을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6NkRyu3aFLq"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53eh8UGsaFLq"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - map()을 사용하여 norm_layer에 적용하는 데모용 데이터 세트를 생성합니다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRqy1-kJaFLq"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(lambda X, y: (norm_layer(X), y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMyqVKPUaFLr"
      },
      "outputs": [],
      "source": [
        "list(dataset.take(1))  # 추가 코드 - 첫 번째 배치 표시"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_FCPzdxaFLr"
      },
      "outputs": [],
      "source": [
        "class MyNormalization(tf.keras.layers.Layer):\n",
        "    def adapt(self, X):\n",
        "        self.mean_ = np.mean(X, axis=0, keepdims=True)\n",
        "        self.std_ = np.std(X, axis=0, keepdims=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        eps = tf.keras.backend.epsilon()  # 0 나눗셈 방지\n",
        "        return (inputs - self.mean_) / (self.std_ + eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDCsE2yVaFLr"
      },
      "outputs": [],
      "source": [
        "my_norm_layer = MyNormalization()\n",
        "my_norm_layer.adapt(X_train)\n",
        "X_train_scaled = my_norm_layer(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGXkmO3MaFLr"
      },
      "source": [
        "## 13.3.2 `Discretization` 층"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMcudGs_aFLr"
      },
      "outputs": [],
      "source": [
        "age = tf.constant([[10.], [93.], [57.], [18.], [37.], [5.]])\n",
        "discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])\n",
        "age_categories = discretize_layer(age)\n",
        "age_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KUDRwCjaFLr"
      },
      "outputs": [],
      "source": [
        "discretize_layer = tf.keras.layers.Discretization(num_bins=3)\n",
        "discretize_layer.adapt(age)\n",
        "age_categories = discretize_layer(age)\n",
        "age_categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uceMIEg4aFLr"
      },
      "source": [
        "## 13.3.3 `CategoryEncoding` 층"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uENH-R-vaFLr"
      },
      "outputs": [],
      "source": [
        "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)\n",
        "onehot_layer(age_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3liig840aFLr"
      },
      "outputs": [],
      "source": [
        "two_age_categories = np.array([[1, 0], [2, 2], [2, 0]])\n",
        "onehot_layer(two_age_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEHCDTwIaFLr"
      },
      "outputs": [],
      "source": [
        "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3, output_mode=\"count\")\n",
        "onehot_layer(two_age_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td7A54oYaFLs"
      },
      "outputs": [],
      "source": [
        "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3 + 3)\n",
        "onehot_layer(two_age_categories + [0, 3])  # 두 번째 특성에 3을 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZy-SPEdaFLs"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 각 기능을 개별적으로 원핫 인코딩하는 다른 방법을 보여줍니다.\n",
        "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3,\n",
        "                                                output_mode=\"one_hot\")\n",
        "tf.keras.layers.concatenate([onehot_layer(cat)\n",
        "                             for cat in tf.transpose(two_age_categories)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFF0TFLPaFLs"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - tf.one_hot() 및 Flatten을 사용하여 이 작업을 수행하는 다른 방법을 보여줍니다.\n",
        "tf.keras.layers.Flatten()(tf.one_hot(two_age_categories, depth=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28Fif7toaFLs"
      },
      "source": [
        "## 13.3.4 `StringLookup` 층"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i0xT3baaFLs"
      },
      "outputs": [],
      "source": [
        "cities = [\"Auckland\", \"Paris\", \"Paris\", \"San Francisco\"]\n",
        "str_lookup_layer = tf.keras.layers.StringLookup()\n",
        "str_lookup_layer.adapt(cities)\n",
        "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOvu0UspaFLs"
      },
      "outputs": [],
      "source": [
        "str_lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=5)\n",
        "str_lookup_layer.adapt(cities)\n",
        "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Foo\"], [\"Bar\"], [\"Baz\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAUTK6ukaFLs"
      },
      "outputs": [],
      "source": [
        "str_lookup_layer = tf.keras.layers.StringLookup(output_mode=\"one_hot\")\n",
        "str_lookup_layer.adapt(cities)\n",
        "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POS-8-RbaFLs"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - IntegerLookup 층을 사용한 예제\n",
        "ids = [123, 456, 789]\n",
        "int_lookup_layer = tf.keras.layers.IntegerLookup()\n",
        "int_lookup_layer.adapt(ids)\n",
        "int_lookup_layer([[123], [456], [123], [111]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2N5XPhlaFLs"
      },
      "source": [
        "## 13.3.5 `Hashing` 층"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJeCIpLGaFLt"
      },
      "outputs": [],
      "source": [
        "hashing_layer = tf.keras.layers.Hashing(num_bins=10)\n",
        "hashing_layer([[\"Paris\"], [\"Tokyo\"], [\"Auckland\"], [\"Montreal\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvJCN5DyaFLt"
      },
      "source": [
        "## 13.3.6 임베딩을 사용해 범주형 특성 인코딩하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pz5OPASaFLt"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)\n",
        "embedding_layer(np.array([2, 4, 2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnsdnRiVaFLt"
      },
      "source": [
        "**경고**: 케라스 2.8.0([이슈 #16101](https://github.com/keras-team/keras/issues/16101)에는 `Sequential` 모델의 첫 번째 레이어로 `StringLookup` 레이어를 사용할 수 없는 버그가 있습니다. 다행히도 간단한 해결 방법이 있습니다. 첫 번째 레이어로 `InputLayer`를 추가하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN3VzWmJaFLt"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "ocean_prox = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
        "str_lookup_layer = tf.keras.layers.StringLookup()\n",
        "str_lookup_layer.adapt(ocean_prox)\n",
        "lookup_and_embed = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=[], dtype=tf.string),  # WORKAROUND\n",
        "    str_lookup_layer,\n",
        "    tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(),\n",
        "                              output_dim=2)\n",
        "])\n",
        "lookup_and_embed(np.array([\"<1H OCEAN\", \"ISLAND\", \"<1H OCEAN\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PDTrFTLaFLt"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 시드 설정 및 랜덤 데이터 생성\n",
        "# (원하는 경우 실제 데이터셋을 로드해도 괜찮습니다.)\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "X_train_num = np.random.rand(10_000, 8)\n",
        "X_train_cat = np.random.choice(ocean_prox, size=10_000)\n",
        "y_train = np.random.rand(10_000, 1)\n",
        "X_valid_num = np.random.rand(2_000, 8)\n",
        "X_valid_cat = np.random.choice(ocean_prox, size=2_000)\n",
        "y_valid = np.random.rand(2_000, 1)\n",
        "\n",
        "num_input = tf.keras.layers.Input(shape=[8], name=\"num\")\n",
        "cat_input = tf.keras.layers.Input(shape=[], dtype=tf.string, name=\"cat\")\n",
        "cat_embeddings = lookup_and_embed(cat_input)\n",
        "encoded_inputs = tf.keras.layers.concatenate([num_input, cat_embeddings])\n",
        "outputs = tf.keras.layers.Dense(1)(encoded_inputs)\n",
        "model = tf.keras.models.Model(inputs=[num_input, cat_input], outputs=[outputs])\n",
        "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
        "history = model.fit((X_train_num, X_train_cat), y_train, epochs=5,\n",
        "                    validation_data=((X_valid_num, X_valid_cat), y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xcMe1QlaFLt"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 모델이 tf.data.Dataset을 사용하여 학습될 수도 있음을 보여줍니다.\n",
        "train_set = tf.data.Dataset.from_tensor_slices(\n",
        "    ((X_train_num, X_train_cat), y_train)).batch(32)\n",
        "valid_set = tf.data.Dataset.from_tensor_slices(\n",
        "    ((X_valid_num, X_valid_cat), y_valid)).batch(32)\n",
        "history = model.fit(train_set, epochs=5,\n",
        "                    validation_data=valid_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqOA1LKbaFLt"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 데이터셋에 딕셔너리가 포함될 수 있음을 보여줍니다.\n",
        "train_set = tf.data.Dataset.from_tensor_slices(\n",
        "    ({\"num\": X_train_num, \"cat\": X_train_cat}, y_train)).batch(32)\n",
        "valid_set = tf.data.Dataset.from_tensor_slices(\n",
        "    ({\"num\": X_valid_num, \"cat\": X_valid_cat}, y_valid)).batch(32)\n",
        "history = model.fit(train_set, epochs=5, validation_data=valid_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcp5vWNSaFLu"
      },
      "source": [
        "## 13.3.7 텍스트 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzxeNj46aFLu"
      },
      "outputs": [],
      "source": [
        "train_data = [\"To be\", \"!(to be)\", \"That's the question\", \"Be, be, be.\"]\n",
        "text_vec_layer = tf.keras.layers.TextVectorization()\n",
        "text_vec_layer.adapt(train_data)\n",
        "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmiW1A-waFLu"
      },
      "outputs": [],
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(ragged=True)\n",
        "text_vec_layer.adapt(train_data)\n",
        "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwbvd7OHaFLu"
      },
      "outputs": [],
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(output_mode=\"tf_idf\")\n",
        "text_vec_layer.adapt(train_data)\n",
        "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfeKZK-4aFLu"
      },
      "outputs": [],
      "source": [
        "2 * np.log(1 + 4 / (1 + 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6JpCYBeaFLu"
      },
      "outputs": [],
      "source": [
        "1 * np.log(1 + 4 / (1 + 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuNgf4ySaFLu",
        "tags": []
      },
      "source": [
        "## 13.3.8 사전 훈련된 언어 모델 구성 요소 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sXiVk_eaFLu"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
        "sentence_embeddings = hub_layer(tf.constant([\"To be\", \"Not to be\"]))\n",
        "sentence_embeddings.numpy().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIuI44QTaFLv"
      },
      "source": [
        "## 13.3.9 이미지 전처리 층"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR2LkobyaFLv"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_sample_images\n",
        "\n",
        "images = load_sample_images()[\"images\"]\n",
        "crop_image_layer = tf.keras.layers.CenterCrop(height=100, width=100)\n",
        "cropped_images = crop_image_layer(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW1hRYCCaFLv"
      },
      "outputs": [],
      "source": [
        "plt.imshow(images[0])\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IR7-z1CaFLv"
      },
      "outputs": [],
      "source": [
        "plt.imshow(cropped_images[0] / 255)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vxv-xhVaFLv"
      },
      "source": [
        "# 13.4 텐서플로 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZFcKrwUaFLv"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "datasets = tfds.load(name=\"mnist\")\n",
        "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV9Jrx5NaFLv"
      },
      "outputs": [],
      "source": [
        "for batch in mnist_train.shuffle(10_000, seed=42).batch(32).prefetch(1):\n",
        "    images = batch[\"image\"]\n",
        "    labels = batch[\"label\"]\n",
        "    # [...] 이미지와 레이블로 무언가를 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mBpjuqvaFLv"
      },
      "outputs": [],
      "source": [
        "mnist_train = mnist_train.shuffle(10_000, seed=42).batch(32)\n",
        "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
        "mnist_train = mnist_train.prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGLytIsNaFLv"
      },
      "outputs": [],
      "source": [
        "train_set, valid_set, test_set = tfds.load(\n",
        "    name=\"mnist\",\n",
        "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
        "    as_supervised=True\n",
        ")\n",
        "train_set = train_set.shuffle(10_000, seed=42).batch(32).prefetch(1)\n",
        "valid_set = valid_set.batch(32).cache()\n",
        "test_set = test_set.batch(32).cache()\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=5)\n",
        "test_loss, test_accuracy = model.evaluate(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6T2L5_laFLv",
        "tags": []
      },
      "source": [
        "# 연습문제\n",
        "\n",
        "## 1. to 8.\n",
        "\n",
        "부록 A 참고"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZXjj47LaFLw"
      },
      "source": [
        "## 9.\n",
        "### a.\n",
        "_문제: (10장에서 소개한) 패션 MNIST 데이터셋을 적재하고 훈련 세트, 검증 세트, 테스트\n",
        "세트로 나눕니다. 훈련 세트를 섞은 다음 각 데이터셋을 TFRecord 파일로 저장합니\n",
        "다. 각 레코드는 두 개의 특성을 가진 `Example` 프로토콜 버퍼, 즉 직렬화된 이미지(`tf.io.serialize_tensor()`를 사용해 이미지를 직렬화하세요)와 레이블입니다. 참고: 용량이 큰 이미지일 경우 `tf.io.encode_jpeg()`를 사용할 수 있습니다. 많은 공간을 절약할 수 있지만 이미지 품질이 손해를 봅니다._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWY1Ckr6aFLw"
      },
      "outputs": [],
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr1UgVVhaFLw"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_set = train_set.shuffle(len(X_train), seed=42)\n",
        "valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
        "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZcKLd2VaFLw"
      },
      "outputs": [],
      "source": [
        "def create_example(image, label):\n",
        "    image_data = tf.io.serialize_tensor(image)\n",
        "    #image_data = tf.io.encode_jpeg(image[..., np.newaxis])\n",
        "    return Example(\n",
        "        features=Features(\n",
        "            feature={\n",
        "                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
        "                \"label\": Feature(int64_list=Int64List(value=[label])),\n",
        "            }))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx_kqjEXaFLw"
      },
      "outputs": [],
      "source": [
        "for image, label in valid_set.take(1):\n",
        "    print(create_example(image, label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la8h4jBkaFLw"
      },
      "source": [
        "다음 함수는 주어진 데이터셋을 일련의 TFRecord 파일로 저장합니다. 이 예제는 라운드-로빈 방식으로 파일에 저장합니다. 이를 위해 `dataset.enumerate()` 메서드로 모든 샘플을 순회하고 저장할 파일을 겨정하기 위해 `index % n_shards`를 계산합니다. 표준 `contextlib.ExitStack` 클래스를 사용해 쓰는 동안 I/O 에러의 발생 여부에 상관없이 모든 `writer`가 적절히 종료되었는지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPvwbArgaFLw"
      },
      "outputs": [],
      "source": [
        "from contextlib import ExitStack\n",
        "\n",
        "def write_tfrecords(name, dataset, n_shards=10):\n",
        "    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n",
        "             for index in range(n_shards)]\n",
        "    with ExitStack() as stack:\n",
        "        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n",
        "                   for path in paths]\n",
        "        for index, (image, label) in dataset.enumerate():\n",
        "            shard = index % n_shards\n",
        "            example = create_example(image, label)\n",
        "            writers[shard].write(example.SerializeToString())\n",
        "    return paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBs5gL-6aFLw"
      },
      "outputs": [],
      "source": [
        "train_filepaths = write_tfrecords(\"my_fashion_mnist.train\", train_set)\n",
        "valid_filepaths = write_tfrecords(\"my_fashion_mnist.valid\", valid_set)\n",
        "test_filepaths = write_tfrecords(\"my_fashion_mnist.test\", test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1liq3tkaFLw"
      },
      "source": [
        "### b.\n",
        "_문제: tf.data로 각 세트를 위한 효율적인 데이터셋을 만듭니다. 마지막으로 이 데이터셋으로\n",
        "입력 특성을 표준화하는 전처리 층을 포함한 케라스 모델을 훈련합니다. 텐서보드로 프\n",
        "로파일 데이터를 시각화하여 가능한 한 입력 파이프라인을 효율적으로 만들어보세요._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plIAMq6UaFLw"
      },
      "outputs": [],
      "source": [
        "def preprocess(tfrecord):\n",
        "    feature_descriptions = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
        "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
        "    #image = tf.io.decode_jpeg(example[\"image\"])\n",
        "    image = tf.reshape(image, shape=[28, 28])\n",
        "    return image, example[\"label\"]\n",
        "\n",
        "def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n",
        "                  n_parse_threads=5, batch_size=32, cache=True):\n",
        "    dataset = tf.data.TFRecordDataset(filepaths,\n",
        "                                      num_parallel_reads=n_read_threads)\n",
        "    if cache:\n",
        "        dataset = dataset.cache()\n",
        "    if shuffle_buffer_size:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hets27CraFLx"
      },
      "outputs": [],
      "source": [
        "train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
        "valid_set = mnist_dataset(valid_filepaths)\n",
        "test_set = mnist_dataset(test_filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv2AkkY-aFLx"
      },
      "outputs": [],
      "source": [
        "for X, y in train_set.take(1):\n",
        "    for i in range(5):\n",
        "        plt.subplot(1, 5, i + 1)\n",
        "        plt.imshow(X[i].numpy(), cmap=\"binary\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(str(y[i].numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPUPla-XaFLx"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "standardization = tf.keras.layers.Normalization(input_shape=[28, 28])\n",
        "\n",
        "sample_image_batches = train_set.take(100).map(lambda image, label: image)\n",
        "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n",
        "                               axis=0).astype(np.float32)\n",
        "standardization.adapt(sample_images)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    standardization,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"nadam\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhW4qEj2aFLx"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "logs = Path() / \"my_logs\" / \"run_\" / datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=logs, histogram_freq=1, profile_batch=10)\n",
        "\n",
        "model.fit(train_set, epochs=5, validation_data=valid_set,\n",
        "          callbacks=[tensorboard_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrwrYklAaFLx"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./my_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtZk1jHVaFLx"
      },
      "source": [
        "## 10.\n",
        "_문제: 이 연습문제에서 데이터셋을 다운로드 및 분할하고 `tf.data.Dataset` 객체를 만들어 데이터를 적재하고 효율적으로 전처리하겠습니다. 그다음 `Embedding` 층을 포함한 이진 분류 모델을 만들고 훈련시킵니다._\n",
        "\n",
        "### a.\n",
        "_문제: [인터넷 영화 데이터베이스](https://imdb.com/)의 영화 리뷰 50,000개를 담은 [영화 리뷰\n",
        "데이터셋](https://homl.info/imdb)을 다운로드합니다. 이 데이터는\n",
        "`train`과 `test`라는 두 개의 디렉터리로 구성되어 있습니다. 각 디렉터리에는 12,500개의 긍정 리뷰를 담은 `pos` 서브디렉터리와 12,500개의 부정 리뷰를 담은 `neg` 서브디렉터리가 있습니다. 리뷰는 각각 별도의 텍스트 파일에 저장되어 있습니다. (전처리된 BOW를 포함해) 다른 파일과 디렉터리가 있지만 이 연습문제에서는 무시합니다._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSFoaXadaFLx"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "root = \"https://ai.stanford.edu/~amaas/data/sentiment/\"\n",
        "filename = \"aclImdb_v1.tar.gz\"\n",
        "filepath = tf.keras.utils.get_file(filename, root + filename, extract=True,\n",
        "                                   cache_dir=\".\")\n",
        "path = Path(filepath).with_name(\"aclImdb\")\n",
        "path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeJ5JoGraFLx"
      },
      "source": [
        "`tree()` 함수를 정의하여 `aclImdb` 디렉터리의 구조를 확인하겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-nz6JE7aFLy"
      },
      "outputs": [],
      "source": [
        "def tree(path, level=0, indent=4, max_files=3):\n",
        "    if level == 0:\n",
        "        print(f\"{path}/\")\n",
        "        level += 1\n",
        "    sub_paths = sorted(path.iterdir())\n",
        "    sub_dirs = [sub_path for sub_path in sub_paths if sub_path.is_dir()]\n",
        "    filepaths = [sub_path for sub_path in sub_paths if not sub_path in sub_dirs]\n",
        "    indent_str = \" \" * indent * level\n",
        "    for sub_dir in sub_dirs:\n",
        "        print(f\"{indent_str}{sub_dir.name}/\")\n",
        "        tree(sub_dir,  level + 1, indent)\n",
        "    for filepath in filepaths[:max_files]:\n",
        "        print(f\"{indent_str}{filepath.name}\")\n",
        "    if len(filepaths) > max_files:\n",
        "        print(f\"{indent_str}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bazE8FiCaFLy"
      },
      "outputs": [],
      "source": [
        "tree(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTUnXxN0aFLy"
      },
      "outputs": [],
      "source": [
        "def review_paths(dirpath):\n",
        "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
        "\n",
        "train_pos = review_paths(path / \"train\" / \"pos\")\n",
        "train_neg = review_paths(path / \"train\" / \"neg\")\n",
        "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
        "test_valid_neg = review_paths(path / \"test\" / \"neg\")\n",
        "\n",
        "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECcMASZxaFLy"
      },
      "source": [
        "### b.\n",
        "_문제: 테스트 세트를 검증 세트(15,000개)와 테스트 세트(10,000개)로 나눕니다._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cdmro9KkaFLy"
      },
      "outputs": [],
      "source": [
        "np.random.shuffle(test_valid_pos)\n",
        "\n",
        "test_pos = test_valid_pos[:5000]\n",
        "test_neg = test_valid_neg[:5000]\n",
        "valid_pos = test_valid_pos[5000:]\n",
        "valid_neg = test_valid_neg[5000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVDuxuTTaFLy"
      },
      "source": [
        "### c.\n",
        "_문제: tf.data를 사용해 각 세트에 대한 효율적인 데이터셋을 만듭니다._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u8Se6gFaFLy"
      },
      "source": [
        "이 데이터셋을 메모리에 적재할 수 있으므로 파이썬 코드와 `tf.data.Dataset.from_tensor_slices()`를 사용해 모든 데이터를 적재합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS58wbQQaFLy"
      },
      "outputs": [],
      "source": [
        "def imdb_dataset(filepaths_positive, filepaths_negative):\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
        "        for filepath in filepaths:\n",
        "            with open(filepath) as review_file:\n",
        "                reviews.append(review_file.read())\n",
        "            labels.append(label)\n",
        "    return tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.constant(reviews), tf.constant(labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH7DDy9GaFLy"
      },
      "outputs": [],
      "source": [
        "for X, y in imdb_dataset(train_pos, train_neg).take(3):\n",
        "    print(X)\n",
        "    print(y)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2swYKdpaFLy"
      },
      "outputs": [],
      "source": [
        "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFQwwxK7aFLy"
      },
      "source": [
        "이 데이터셋을 적재하고 10회 반복하는데 약 17초가 걸립니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfFSQlVYaFLy"
      },
      "source": [
        "하지만 이 데이터셋이 메모리에 맞지 않는다고 가정하고 좀 더 재미있는 것을 만들어 보죠. 다행히 각 리뷰는 한 줄로 되어 있기 때문에(`<br />`로 줄바꿈됩니다) `TextLineDataset`를 사용해 리뷰를 읽을 수 있습니다. 그렇지 않으면 입력 파일을 전처리해야 합니다(예를 들어, TFRecord로 바꿉니다). 매우 큰 데이터셋의 경우 아파치 빔(Apache Beam) 같은 도구를 사용하는 것이 합리적입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z-mcbldaFLz"
      },
      "outputs": [],
      "source": [
        "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
        "    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n",
        "                                          num_parallel_reads=n_read_threads)\n",
        "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
        "    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n",
        "                                          num_parallel_reads=n_read_threads)\n",
        "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
        "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT8mwCFYaFLz"
      },
      "outputs": [],
      "source": [
        "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FxllbWxaFLz"
      },
      "source": [
        "이 데이터셋을 10회 반복하는데 33초 걸립니다. 데이터셋이 RAM에 캐싱되지 않고 에포크마다 다시 로드되기 때문에 매우 느립니다. `.repeat(10)` 전에 `.cache()`를 추가하면 이전만큼 빨라지는 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NOc_mJPaFLz"
      },
      "outputs": [],
      "source": [
        "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).cache().repeat(10): pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13anppEhaFLz"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000, seed=42)\n",
        "train_set = train_set.batch(batch_size).prefetch(1)\n",
        "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
        "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlxyn5usaFLz"
      },
      "source": [
        "### d.\n",
        "_문제: 리뷰를 전처리하기 위해 `TextVectorization` 층을 사용한 이진 분류 모델을 만드세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIxz-SUWaFLz"
      },
      "source": [
        "`TextVectorization` 층을 만들고 이를 전체 IMDB 훈련 세트에 적용해 보겠습니다(훈련 세트가 RAM에 맞지 않는 경우 `train_set.take(500)`를 호출하여 더 작은 샘플의 훈련 세트를 사용할 수 있습니다). 지금은 TF-IDF를 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWG_h7PCaFLz"
      },
      "outputs": [],
      "source": [
        "max_tokens = 1000\n",
        "sample_reviews = train_set.map(lambda review, label: review)\n",
        "text_vectorization = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=max_tokens, output_mode=\"tf_idf\")\n",
        "text_vectorization.adapt(sample_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SXkFA80aFLz"
      },
      "source": [
        "좋아요! 이제 어휘사전에 있는 처음 10개의 단어를 살펴봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vicbdwstaFLz"
      },
      "outputs": [],
      "source": [
        "text_vectorization.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa6AU5XlaFLz"
      },
      "source": [
        "리뷰에서 가장 많이 사용되는 단어입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqjBxiUPaFLz"
      },
      "source": [
        "모델을 훈련할 준비가 되었습니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmXteU3TaFLz"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    text_vectorization,\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_set, epochs=5, validation_data=valid_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_Q3cpw5aFL0"
      },
      "source": [
        "첫 번째 에포크에서 검증 세트에 대해 84.2%의 정확도를 얻었습니다. 하지만 더 진전이 없습니다. 16장에서 이를 더 개선해 보겠습니다. 지금은 `tf.data`와 케라스 전처리 층으로 효율적인 전처리를 수행하는 것에만 초점을 맞추었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG5JaQFyaFL0"
      },
      "source": [
        "### e.\n",
        "_문제: `Embedding` 층을 추가하고 단어 개수의 제곱근을 곱하여 리뷰마다 평균 임베딩을 계산하세요(16장 참조). 이제 스케일이 조정된 이 평균 임베딩을 모델의 다음 부분으로 전달할 수 있습니다._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40dmRmD4aFL0"
      },
      "source": [
        "각 리뷰의 평균 임베딩을 계산하고 리뷰에 있는 단어 개수의 제곱근을 곱하기 위해 간단한 함수를 정의합니다. 각 문장에 대해서 이 함수는 $M \\times \\sqrt N$을 계산합니다. 여기에서 $M$은 (패딩 토큰을 제외하고) 문장에 있는 모든 단어 임베딩의 평균입니다. $N$은 (패딩 토큰을 제외한) 문장에 있는 단어의 개수입니다. $M$을 $\\dfrac{S}{N}$로 다시 쓸 수 있습니다. 여기에서 $S$는 모든 단어 임베딩의 합입니다(패딩 토큰은 0 벡터이므로 합에서는 패딩 토큰을 포함했는지 여부가 문제가 안됩니다). 따라서 이 함수는 $M \\times \\sqrt N = \\dfrac{S}{N} \\times \\sqrt N = \\dfrac{S}{\\sqrt N \\times \\sqrt N} \\times \\sqrt N= \\dfrac{S}{\\sqrt N}$를 반환해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDNp6MjDaFL0"
      },
      "outputs": [],
      "source": [
        "def compute_mean_embedding(inputs):\n",
        "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
        "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)\n",
        "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
        "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words\n",
        "\n",
        "another_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n",
        "                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n",
        "compute_mean_embedding(another_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsPQr26zaFL0"
      },
      "source": [
        "결과가 올바른지 확인해 보죠. 첫 번째 리뷰에는 2개의 단어가 있습니다(마지막 토큰은 `<pad>` 토큰을 나타내는 0벡터입니다). 이 두 단어의 평균 임베딩을 계산하고 그 결과에 2의 제곱근을 곱해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Y34M4caFL0"
      },
      "outputs": [],
      "source": [
        "tf.reduce_mean(another_example[0:1, :2], axis=1) * tf.sqrt(2.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUaFYrBxaFL0"
      },
      "source": [
        "좋습니다. 두 번째 리뷰를 확인해 보죠. 이 리뷰는 하나의 단어만 가지고 있습니다(두 개의 패딩 토큰은 무시합니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d6EAGz0aFL0"
      },
      "outputs": [],
      "source": [
        "tf.reduce_mean(another_example[1:2, :1], axis=1) * tf.sqrt(1.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bEnBpXBaFL0"
      },
      "source": [
        "완벽합니다. 이제 최종 모델을 훈련할 준비가 되었습니다. TF-IDF를 순서가 있는 인코딩(`output_mode=\"int\"`)으로 대체한 다음 `Embedding` 층을 그다음에 두고 `compute_mean_embedding` 층을 호출하는 `Lambda` 층을 두는 것을 제외하고는 이전과 동일합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlLPHDm6aFL0"
      },
      "outputs": [],
      "source": [
        "embedding_size = 20\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "text_vectorization = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=max_tokens, output_mode=\"int\")\n",
        "text_vectorization.adapt(sample_reviews)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    text_vectorization,\n",
        "    tf.keras.layers.Embedding(input_dim=max_tokens,\n",
        "                              output_dim=embedding_size,\n",
        "                              mask_zero=True),  # <pad> tokens => zero vectors\n",
        "    tf.keras.layers.Lambda(compute_mean_embedding),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HCX2Na_aFL0"
      },
      "source": [
        "### f.\n",
        "_문제: 모델을 훈련하고 얼마의 정확도가 나오는지 확인해보세요. 가능한 한 훈련 속도를 빠르게 하기 위해 파이프라인을 최적화해보세요._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgFsHGMxaFL0"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_set, epochs=5, validation_data=valid_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42H8hT0qaFL1"
      },
      "source": [
        "임베딩을 사용해서 더 나아지지 않았습니다(16장에서 이를 개선해 보겠습니다). 파이프라인은 충분히 빨라 보입니다(앞서 최적화했습니다)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRi6YuWwaFL1"
      },
      "source": [
        "### g.\n",
        "_문제: `tfds.load(\"imdb_reviews\")`와 같이 TFDS를 사용해 동일한 데이터셋을 간단하게 적재해보세요._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc_REB_YaFL1"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "datasets = tfds.load(name=\"imdb_reviews\")\n",
        "train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrBV78CgaFL1"
      },
      "outputs": [],
      "source": [
        "for example in train_set.take(1):\n",
        "    print(example[\"text\"])\n",
        "    print(example[\"label\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "nav_menu": {
      "height": "264px",
      "width": "369px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}